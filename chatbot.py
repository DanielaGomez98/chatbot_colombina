import streamlit as st
from llm.FAQ.faq_ollama import generate_faqs
from llm.QA.qa_ollama import process_question
from llm.llm_openai import procesar_pregunta_colombina
from llm.summary.generate_summary import generate_summary

st.set_page_config(page_title="Centro de InformaciÃ³n Colombina", page_icon="ğŸ­", layout="wide")

st.title("ğŸ­ Centro de InformaciÃ³n Colombina")
    
st.markdown("""
**Bienvenido al Centro de InformaciÃ³n integral de Colombina**

Explora diferentes aspectos de la informaciÃ³n corporativa a travÃ©s de las siguientes herramientas:
""")

# Crear pestaÃ±as
tab1, tab2, tab3, tab4 = st.tabs([
    "ğŸ” Consulta Q&A", 
    "â“ Preguntas Frecuentes", 
    "ğŸ“‹ Resumen Ejecutivo", 
    "ğŸ’¬ Chatbot Interactivo"
])

with st.sidebar:
    st.header("ğŸ› ï¸ ParÃ¡metros de ConfiguraciÃ³n")
    
    temperature = st.slider(
        "ğŸŒ¡ï¸ Temperatura",
        min_value=0.0,
        max_value=1.0,
        value=0.5,
        step=0.1,
        help="Controla la creatividad de las respuestas. Valores mÃ¡s altos = mÃ¡s creatividad"
    )
    
    top_p = st.slider(
        "ğŸ¯ Top P",
        min_value=0.0,
        max_value=1.0,
        value=0.9,
        step=0.05,
        help="Controla la diversidad de las respuestas. Valores mÃ¡s bajos = mÃ¡s enfoque"
    )
    
    st.divider()

    model_choice = st.selectbox(
        "**Modelo Ollama**",
        ["gpt-oss:20b"],
        index=0
    )
    
    # if st.session_state.get("messages", []):
    #     if st.button("ğŸ—‘ï¸ Limpiar ConversaciÃ³n", 
    #                 type="secondary", 
    #                 help="Borrar todo el historial de la conversaciÃ³n",
    #                 use_container_width=True):
    #         st.session_state.messages = []
    #         st.rerun()

with tab1:
    st.subheader("ğŸ” Sistema de Consulta Q&A")
    st.markdown("""
    Realiza preguntas especÃ­ficas sobre Colombina y obtÃ©n respuestas basadas en la base de conocimiento oficial.
    """)
    
    st.markdown("ğŸš€ **Preguntas de Ejemplo**")
    example_questions = [
        "Â¿En quÃ© aÃ±o se creÃ³ el Bon Bon Bum?",
        "Â¿CuÃ¡l es el programa de Colombina para acompaÃ±ar a sus proveedores?",
        "Â¿QuÃ© porcentaje de energÃ­a renovable utiliza Colombina en Colombia?",
        "Â¿CuÃ¡les son las plantas de producciÃ³n de Colombina?",
        "Â¿QuÃ© es Colombina EnergÃ­a S.A.S. E.S.P.?",
    ]
    
    selected_question = st.selectbox("Selecciona una pregunta de ejemplo:", example_questions)

    question = selected_question
    
    if st.button("ğŸ” Buscar Respuesta", disabled=not question):
        with st.spinner("Procesando pregunta..."):
            try:
                answer = process_question(model_choice, question, temperature, top_p)
                
                st.success("âœ… Respuesta encontrada:")
                st.write(answer)
                
            except Exception as e:
                st.error(f"âŒ Error procesando pregunta: {e}")


with tab2:
    st.subheader("â“ Generador de Preguntas Frecuentes")
    st.markdown("""
    **Genera automÃ¡ticamente preguntas frecuentes basadas en la informaciÃ³n disponible de Colombina.**
    
    Esta herramienta analiza toda la base de conocimiento y crea preguntas que comÃºnmente 
    podrÃ­an hacer clientes, proveedores o colaboradores.
    """)
    
    if st.button("ğŸ¯ Generar FAQs"):
        with st.spinner("Analizando base de conocimiento y generando preguntas frecuentes..."):
            try:
                faqs = generate_faqs(model_choice, temperature, top_p)
                
                st.success("âœ… Preguntas frecuentes generadas:")
                st.markdown(faqs)
                
                st.download_button(
                    label="ğŸ“¥ Descargar FAQs",
                    data=faqs,
                    file_name="faqs_colombina.txt",
                    mime="text/plain"
                )
                
            except Exception as e:
                st.error(f"âŒ Error generando FAQs: {e}")


with tab3:
        st.subheader("ğŸ“‹ Generador de Resumen Ejecutivo")
        st.markdown("""
        **Genera un resumen ejecutivo completo de toda la informaciÃ³n disponible sobre Colombina.**
        
        Este resumen incluye aspectos clave como historia, productos, sostenibilidad, 
        operaciones y logros principales de la empresa.
        """)
        
        if st.button("ğŸ“Š Generar Resumen Ejecutivo"):
            with st.spinner("Analizando informaciÃ³n y generando resumen ejecutivo..."):
                try:
                    summary = generate_summary(model_choice, temperature, top_p)

                    st.success("âœ… Resumen ejecutivo generado:")
                    st.markdown(summary)
                    
                    st.download_button(
                        label="ğŸ“¥ Descargar Resumen",
                        data=summary,
                        file_name="resumen_ejecutivo_colombina.txt",
                        mime="text/plain"
                    )
                    
                except Exception as e:
                    st.error(f"âŒ Error generando resumen: {e}")


with tab4:
    st.markdown("""
    Â¡Bienvenido al asistente virtual de **Colombina**! 

    **Â¿CÃ³mo funciona?**
    - ğŸ’¬ **ConversaciÃ³n simple**: Haz preguntas sobre productos, servicios o informaciÃ³n de Colombina
    - ğŸ¬ **Conocimiento especializado**: InformaciÃ³n actualizada sobre dulces, chocolates y productos Colombina
    - âš¡ **Respuestas rÃ¡pidas**: ObtÃ©n informaciÃ³n instantÃ¡nea sobre lo que necesites

    **Â¡Comienza** preguntando sobre productos, ingredientes, disponibilidad o cualquier tema relacionado con Colombina!
    """)

    if "messages" not in st.session_state:
        st.session_state.messages = []

    for msg in st.session_state.messages:
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"])

    user_input = st.chat_input("PregÃºntame sobre productos Colombina, ingredientes, disponibilidad... ğŸ­")

    if user_input:
        st.session_state.messages.append({"role": "user", "content": user_input})
        
        with st.chat_message("user"):
            st.markdown(user_input)
        
        with st.spinner("Pensando..."):
            try:
                respuesta = procesar_pregunta_colombina(user_input, temperature, top_p)
                
                with st.chat_message("assistant"):
                    st.markdown(respuesta)
                
                st.session_state.messages.append({"role": "assistant", "content": respuesta})
                
            except Exception as e:
                error_message = f"âš ï¸ OcurriÃ³ un error: {e}"
                
                with st.chat_message("assistant"):
                    st.markdown(error_message)
                
                st.session_state.messages.append({"role": "assistant", "content": error_message})