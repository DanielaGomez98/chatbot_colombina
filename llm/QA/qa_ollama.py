import sys
import json
from pathlib import Path
from langchain_ollama import OllamaLLM
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser

project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from utils.logging_util.logger import get_logger

logger = get_logger()
chunks_path = project_root / "utils" / "chunking" / "chunks.json"


def load_knowledge_base(filepath=chunks_path):
    """
    Carga los chunks desde el archivo JSON y los concatena en un solo
    string de texto que servir√° como contexto completo.
    """
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        full_context = ""
        for item in data:
            title = item['metadata'].get('title', 'Fuente desconocida')
            source_file = item['metadata'].get('source_file', 'Documento sin nombre')
            
            full_context += f"--- Inicio del Documento: {title} (Archivo: {source_file}) ---\n"
            full_context += item['content']
            full_context += f"\n--- Fin del Documento: {title} ---\n\n"
            
        logger.info(f"‚úÖ Base de conocimiento cargada y consolidada. Total de caracteres: {len(full_context)}")
        return full_context
    
    except FileNotFoundError:
        logger.error(f"‚ùå Error: El archivo '{filepath}' no fue encontrado.")
        return None
    
    except Exception as e:
        logger.error(f"‚ùå Ocurri√≥ un error al cargar la base de conocimiento: {e}")
        return None


def create_simple_qa_chain(llm_model, temperature, top_p):
    """
    Crea una cadena de Q&A simple que inyecta todo el conocimiento
    en el prompt, sin usar un retriever.
    """
    logger.info(f"\nüîÑ Configurando la cadena Q&A con el modelo de Ollama: '{llm_model}'...")

    llm = OllamaLLM(model=llm_model, temperature=temperature, top_p=top_p)

    template = """
    Eres un asistente experto de la empresa Colombina. Responde la pregunta del usuario bas√°ndote estricta y √∫nicamente en la siguiente base de conocimiento.
    Si la respuesta no se encuentra en la base de conocimiento, responde exactamente: "La informaci√≥n solicitada no se encuentra en mi base de conocimiento."
    No intentes inventar una respuesta. S√© conciso y directo.

    --- INICIO DE LA BASE DE CONOCIMIENTO ---
    {context}
    --- FIN DE LA BASE DE CONOCIMIENTO ---

    Pregunta del usuario:
    {question}

    Respuesta:
    """
    prompt = PromptTemplate.from_template(template)

    simple_qa_chain = (
        prompt
        | llm
        | StrOutputParser()
    )
    
    logger.info("‚úÖ Cadena de Q&A simple configurada exitosamente.")
    return simple_qa_chain


def process_question(llm_model, question, temperature, top_p):
    knowledge_context = load_knowledge_base()
    if knowledge_context is None:
        return

    qa_chain = create_simple_qa_chain(llm_model, temperature, top_p)

    answer = qa_chain.invoke({"context": knowledge_context, "question": question})

    return answer


def main(temperature=0.1, top_p=0.9):
    knowledge_context = load_knowledge_base()
    if knowledge_context is None:
        return

    qa_chain = create_simple_qa_chain(llm_model="gpt-oss:20b", temperature=temperature, top_p=top_p)

    questions = [
        "¬øEn qu√© a√±o se cre√≥ el Bon Bon Bum?",
        "¬øC√≥mo se llama el programa de Colombina para acompa√±ar a sus proveedores y emprendedores?",
        "¬øQu√© porcentaje de la energ√≠a el√©ctrica que utiliza Colombina en sus operaciones en Colombia proviene de fuentes renovables?",
        "¬øCu√°l es la certificaci√≥n que han recibido las 5 f√°bricas de Colombia en relaci√≥n con la gesti√≥n de residuos?",
        "¬øQui√©n fue el fundador de Colombina?",
        "Describe la colaboraci√≥n entre Bon Bon Bum y Taj√≠n. ¬øQu√© producto lanzaron y cu√°les eran las proyecciones de ventas?",
        "¬øCu√°les son los principales logros de Colombina en materia de sostenibilidad relacionados con la energ√≠a y el agua?",
        "Seg√∫n la pol√≠tica de protecci√≥n de datos, ¬øcu√°l es el procedimiento que debe seguir una persona si desea actualizar o rectificar su informaci√≥n personal?",
        "Resume la historia de la creaci√≥n de la chupeta Bon Bon Bum. ¬øQui√©n la cre√≥ y cu√°l fue su innovaci√≥n principal?",
        "¬øQu√© es Colombina Energ√≠a S.A.S. E.S.P. y cu√°l es su funci√≥n principal?",
        "Compara las alianzas de Colombina con Ramo y Postob√≥n. ¬øQu√© productos ic√≥nicos se crearon en cada colaboraci√≥n?",
        "¬øQu√© relaci√≥n existe entre la certificaci√≥n 'Sello Oro Equipares' y los valores corporativos de Colombina?",
        "Lista tres plantas de producci√≥n de Colombina, su ubicaci√≥n y qu√© tipo de productos se fabrican en cada una.",
        "¬øEn qu√© pa√≠ses fuera de Colombia tiene Colombina plantas de producci√≥n, seg√∫n la informaci√≥n proporcionada?",
        "Si soy un proveedor y no he logrado obtener mi certificado de retenci√≥n a trav√©s del portal, ¬øa qu√© correo electr√≥nico debo escribir?",
        "¬øCu√°l es la pol√≠tica de Colombina respecto al uso de huevos libres de jaula y cu√°l es la meta para 2025?",
        "De acuerdo con la pol√≠tica de tratamiento de datos, ¬øqu√© ocurre con la informaci√≥n de un candidato que no es seleccionado para un puesto de trabajo?",
        "¬øCu√°l fue la calificaci√≥n que recibi√≥ Colombina al obtener la certificaci√≥n Basura Cero ORO para sus plantas de helados?",
        "Menciona dos deportistas que han sido embajadores o imagen de campa√±as relacionadas con Bon Bon Bum.",
        "¬øCu√°l es el salario anual del presidente de Colombina?"
    ]

    logger.info("\n--- üöÄ INICIANDO EVALUACI√ìN DEL SISTEMA Q&A (M√âTODO IN-CONTEXT) üöÄ ---\n")
    
    for i, question in enumerate(questions, 1):
        logger.info(f"--- Pregunta {i}/{len(questions)} ---")
        logger.info(f"‚ùì: {question}")
        
        answer = qa_chain.invoke({"context": knowledge_context, "question": question})
        
        logger.info(f"ü§ñ: {answer}")
        logger.info("-" * (len(str(i)) + len(str(len(questions))) + 16))
        logger.info("\n")


if __name__ == "__main__":
    main()