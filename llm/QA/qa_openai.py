import os
import sys
import json
from pathlib import Path

project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from logging_util.logger import get_logger
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser

load_dotenv()
chunks_path = project_root / "chunking" / "chunks.json"

logger = get_logger()


def load_knowledge_base(filepath=chunks_path):
    """
    Carga los chunks desde el archivo JSON y los concatena en un solo
    string de texto que servir√° como contexto completo.
    """
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        full_context = ""
        for item in data:
            title = item['metadata'].get('title', 'Fuente desconocida')
            source_file = item['metadata'].get('source_file', 'Documento sin nombre')
            
            full_context += f"--- Inicio del Documento: {title} (Archivo: {source_file}) ---\n"
            full_context += item['content']
            full_context += f"\n--- Fin del Documento: {title} ---\n\n"
            
        logger.info(f"‚úÖ Base de conocimiento cargada y consolidada. Total de caracteres: {len(full_context)}")
        return full_context
    except FileNotFoundError:
        logger.error(f"‚ùå Error: El archivo '{filepath}' no fue encontrado.")
        return None
    except Exception as e:
        logger.error(f"‚ùå Ocurri√≥ un error al cargar la base de conocimiento: {e}")
        return None


def create_simple_qa_chain(llm_model="gpt-4o"):
    """
    Crea una cadena de Q&A simple usando OpenAI.
    """
    logger.info(f"\nüîÑ Configurando la cadena Q&A con OpenAI: '{llm_model}'...")
    
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        logger.error("‚ùå Falta la API Key de OpenAI")
        return None

    llm = ChatOpenAI(
        model=llm_model,
        temperature=0.1,
        top_p=0.9,
        api_key=api_key,
        max_tokens=1024
    )

    template = """
    Eres un asistente experto de la empresa Colombina. Responde la pregunta del usuario bas√°ndote estricta y √∫nicamente en la siguiente base de conocimiento.
    Si la respuesta no se encuentra en la base de conocimiento, responde exactamente: "La informaci√≥n solicitada no se encuentra en mi base de conocimiento."
    No intentes inventar una respuesta. S√© conciso y directo.

    --- INICIO DE LA BASE DE CONOCIMIENTO ---
    {context}
    --- FIN DE LA BASE DE CONOCIMIENTO ---

    Pregunta del usuario:
    {question}

    Respuesta:
    """
    prompt = PromptTemplate.from_template(template)

    simple_qa_chain = (
        prompt
        | llm
        | StrOutputParser()
    )
    
    logger.info("‚úÖ Cadena de Q&A simple configurada exitosamente.")
    return simple_qa_chain

def main():
    knowledge_context = load_knowledge_base()
    if knowledge_context is None:
        return

    qa_chain = create_simple_qa_chain(llm_model="gpt-4o")
    if qa_chain is None:
        return

    questions = [
        "¬øEn qu√© a√±o se cre√≥ el Bon Bon Bum?",
        "¬øC√≥mo se llama el programa de Colombina para acompa√±ar a sus proveedores y emprendedores?",
        "¬øQu√© porcentaje de la energ√≠a el√©ctrica que utiliza Colombina en sus operaciones en Colombia proviene de fuentes renovables?",
        "¬øCu√°l es la certificaci√≥n que han recibido las 5 f√°bricas de Colombia en relaci√≥n con la gesti√≥n de residuos?",
        "¬øQui√©n fue el fundador de Colombina?",
        "Describe la colaboraci√≥n entre Bon Bon Bum y Taj√≠n. ¬øQu√© producto lanzaron y cu√°les eran las proyecciones de ventas?",
        "¬øCu√°les son los principales logros de Colombina en materia de sostenibilidad relacionados con la energ√≠a y el agua?",
        "Seg√∫n la pol√≠tica de protecci√≥n de datos, ¬øcu√°l es el procedimiento que debe seguir una persona si desea actualizar o rectificar su informaci√≥n personal?",
        "Resume la historia de la creaci√≥n de la chupeta Bon Bon Bum. ¬øQui√©n la cre√≥ y cu√°l fue su innovaci√≥n principal?",
        "¬øQu√© es Colombina Energ√≠a S.A.S. E.S.P. y cu√°l es su funci√≥n principal?",
        "Compara las alianzas de Colombina con Ramo y Postob√≥n. ¬øQu√© productos ic√≥nicos se crearon en cada colaboraci√≥n?",
        "¬øQu√© relaci√≥n existe entre la certificaci√≥n 'Sello Oro Equipares' y los valores corporativos de Colombina?",
        "Lista tres plantas de producci√≥n de Colombina, su ubicaci√≥n y qu√© tipo de productos se fabrican en cada una.",
        "¬øEn qu√© pa√≠ses fuera de Colombia tiene Colombina plantas de producci√≥n, seg√∫n la informaci√≥n proporcionada?",
        "Si soy un proveedor y no he logrado obtener mi certificado de retenci√≥n a trav√©s del portal, ¬øa qu√© correo electr√≥nico debo escribir?",
        "¬øCu√°l es la pol√≠tica de Colombina respecto al uso de huevos libres de jaula y cu√°l es la meta para 2025?",
        "De acuerdo con la pol√≠tica de tratamiento de datos, ¬øqu√© ocurre con la informaci√≥n de un candidato que no es seleccionado para un puesto de trabajo?",
        "¬øCu√°l fue la calificaci√≥n que recibi√≥ Colombina al obtener la certificaci√≥n Basura Cero ORO para sus plantas de helados?",
        "Menciona dos deportistas que han sido embajadores o imagen de campa√±as relacionadas con Bon Bon Bum.",
        "¬øCu√°l es el salario anual del presidente de Colombina?"
    ]

    logger.info("\n--- üöÄ INICIANDO EVALUACI√ìN DEL SISTEMA Q&A (M√âTODO IN-CONTEXT) üöÄ ---\n")
    
    for i, question in enumerate(questions, 1):
        logger.info(f"--- Pregunta {i}/{len(questions)} ---")
        logger.info(f"‚ùì: {question}")
        
        answer = qa_chain.invoke({"context": knowledge_context, "question": question})
        
        logger.info(f"ü§ñ: {answer}")
        logger.info("-" * (len(str(i)) + len(str(len(questions))) + 16))
        logger.info("\n")

if __name__ == "__main__":
    main()



# def procesar_pregunta_colombina(pregunta: str, temperatura: float = 0.5, top_p: float = 0.9):
#     """
#     Procesa la pregunta del usuario usando el modelo OpenAI con par√°metros configurables.
    
#     Args:
#         pregunta (str): La pregunta del usuario
#         temperatura (float): Controla la creatividad (0.0 - 1.0)
#         top_p (float): Controla la diversidad (0.0 - 1.0)
    
#     Returns:
#         str: Respuesta del modelo
#     """
#     try:
#         api_key = os.getenv("OPENAI_API_KEY")
#         if not api_key:
#             return """‚ö†Ô∏è **Falta la API Key de OpenAI**
            
# Para configurar tu API Key:
# 1. Obt√©n tu API Key en: https://platform.openai.com/api-keys
# 2. Crea un archivo `.env` en la carpeta del proyecto
# 3. Agrega: `OPENAI_API_KEY=tu_api_key_aqui`
# 4. O ejecuta: `export OPENAI_API_KEY=tu_api_key_aqui`"""
        
#         llm = ChatOpenAI(
#             model="gpt-4o",
#             temperature=temperatura,
#             top_p=top_p,
#             api_key=api_key
#         )
        
#         template = """Eres un asistente virtual especializado en la empresa Colombina, l√≠der en dulces y confiter√≠a en Colombia y Latinoam√©rica. 
        
# Responde de manera amigable, profesional y con conocimiento sobre:
# - Productos Colombina (dulces, chocolates, confiter√≠a)
# - Historia y valores de la empresa
# - Disponibilidad de productos
# - Ingredientes y informaci√≥n nutricional
# - Promociones y novedades

# Pregunta: {question}
# Respuesta:"""
        
#         prompt = PromptTemplate.from_template(template)
        
#         chain = prompt | llm
        
#         respuesta = chain.invoke({"question": pregunta})
        
#         return respuesta.content.strip()
        
#     except Exception as e:
#         return f"‚ö†Ô∏è Error al procesar la pregunta: {str(e)}"