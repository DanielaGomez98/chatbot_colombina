import json
from langchain_community.llms import Ollama
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser


# ==========================================================
# 1Ô∏è‚É£ Cargar y limitar la base de conocimiento
# ==========================================================
def load_knowledge_base(filepath="chunks.json", max_chunks=25):
    """
    Carga el archivo con los chunks del conocimiento y selecciona los m√°s relevantes
    para evitar exceder el l√≠mite de contexto del modelo.
    """
    with open(filepath, 'r', encoding='utf-8') as f:
        data = json.load(f)

    # Selecciona los fragmentos m√°s largos (los m√°s informativos)
    selected = sorted(data, key=lambda x: len(x["content"]), reverse=True)[:max_chunks]
    context = "\n".join([item["content"] for item in selected])

    print(f"‚úÖ Base de conocimiento cargada. {len(selected)} fragmentos seleccionados de {len(data)} totales.")
    print(f"üìè Tama√±o aproximado del contexto: {len(context.split())} palabras.\n")
    return context


# ==========================================================
# 2Ô∏è‚É£ Crear la cadena de generaci√≥n de preguntas FAQ
# ==========================================================
def create_faq_chain(knowledge_context, llm_model="gpt-oss:20b"):
    """
    Crea una cadena LangChain que genera autom√°ticamente preguntas frecuentes (FAQ)
    sobre Colombina, basadas en la base de conocimiento.
    """
    llm = Ollama(model=llm_model)

    template = """
    Eres un generador de Preguntas Frecuentes (FAQ) sobre la empresa **Colombina**, 
    un l√≠der latinoamericano en alimentos. Tu tarea es analizar la base de conocimiento 
    provista y producir directamente entre 15 y 25 preguntas realistas y diversas 
    que un cliente, proveedor o colaborador podr√≠a hacer sobre la empresa.

    Reglas:
    - Las preguntas deben centrarse en Colombina: su historia, productos, sostenibilidad, 
      innovaci√≥n, programas sociales, proveedores y contacto.
    - No generes preguntas gen√©ricas sobre liderazgo o gesti√≥n.
    - No pidas aclaraciones al usuario ni solicites m√°s informaci√≥n.
    - No incluyas respuestas ni formato adicional, solo las preguntas enumeradas.
    - Redacta las preguntas en tono formal y natural, como en una secci√≥n de FAQ corporativa.

    --- INICIO DE LA BASE DE CONOCIMIENTO ---
    {context}
    --- FIN DE LA BASE DE CONOCIMIENTO ---

    Preguntas frecuentes:
    """

    prompt = PromptTemplate.from_template(template)
    chain = prompt | llm | StrOutputParser()
    return chain


# ==========================================================
# 3Ô∏è‚É£ Funci√≥n principal
# ==========================================================
def main():
    # Cargar base de conocimiento limitada
    context = load_knowledge_base(filepath="chunks.json", max_chunks=25)

    # Crear la cadena FAQ
    faq_chain = create_faq_chain(context, llm_model="gpt-oss:20b")

    print("\n--- üöÄ GENERANDO PREGUNTAS FRECUENTES DE COLOMBINA ---\n")

    # Invocar el modelo
    faqs = faq_chain.invoke({"context": context})
    print(faqs)

    # Guardar resultado
    output_file = "faqs_colombina.txt"
    with open(output_file, "w", encoding="utf-8") as f:
        f.write(faqs)

    print(f"\n‚úÖ Preguntas frecuentes guardadas en '{output_file}'.")


# ==========================================================
# 4Ô∏è‚É£ Ejecuci√≥n directa
# ==========================================================
if __name__ == "__main__":
    main()